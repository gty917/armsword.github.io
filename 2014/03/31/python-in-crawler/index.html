<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>使用python编写简单网络爬虫技巧总结 | armsword的涅槃之地</title>
  <meta name="author" content="armsword">
  
  <meta name="description" content="程序员,搜索引擎,网络编程,后台开发,北邮人,阿里巴巴,神马搜索,Linux,C,C++,Python">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="使用python编写简单网络爬虫技巧总结"/>
  <meta property="og:site_name" content="armsword的涅槃之地"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.ico" rel="icon" type="image/x-ico"> 
  <link rel="alternate" href="/atom.xml" title="armsword的涅槃之地" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">armsword的涅槃之地</a></h1>
  <h2><a href="/">莫思身外无穷事，且读生平未见书 || 不为无益之事，何以遣有涯之生</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about-me">About</a></li>
    
      <li><a href="/document-information">文档资料</a></li>
    
  <li><a href="/atom.xml">RSS</a><li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-03-31T10:53:39.000Z"><a href="/2014/03/31/python-in-crawler/">3月 31 2014</a></time>
      
      
  
    <h1 class="title">使用python编写简单网络爬虫技巧总结</h1>
  

    </header>
    <div class="entry">
      
        <p>使用python写过几个小玩具，基本上都与爬虫、Web相关的，自己也积累了下经验，也看过很多文章，一直想总结下，可惜现在要忙于找工作，实验室活也比较多，也就落下了。感觉如果时间长不记录，也就懒散了，并且许多东西我写完过个两天，我自己都记不住当时怎么想的了。</p>
<p>0、HTTP协议</p>
<p>基本上常见的Web开发里，Web内容都是通过HTTP协议进行传输的（虽然咱不懂Web开发，但是基本的计算机网络知识还是了解的），通过TCP连接服务器的80端口，爬虫其实质就是通过模拟浏览器发送HTTP请求，至于HTTP请求相关知识，点击<a href="http://zh.wikipedia.org/wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE" target="_blank" rel="external">这里</a>。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">HTTP通常通过创建到服务器<span class="number">80</span>端口的TCP连接进行通信</div><div class="line">HTTP协议的内容包括请求方式（<span class="function"><span class="keyword">method</span>）, <span class="title">url</span>，<span class="title">header</span>，<span class="title">body</span>，通常以纯文本方式发送</span></div><div class="line"><span class="title">HTTP</span>返回内容包括状态码，<span class="title">header</span>，<span class="title">body</span>，通常以纯文本方式返回</div><div class="line"><span class="title">header</span>以及<span class="title">body</span>间以<span class="title">CRLF</span><span class="params">(\r\n)</span>分割</div></pre></td></tr></table></figure>

<p>1、最基础的抓取网站内容</p>
<p>使用python编写一个网络爬虫是非常简单的，如下例所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">content = urllib2.urlopen(<span class="string">'http://armsword.com'</span>).<span class="keyword">read</span>()</div></pre></td></tr></table></figure>

<p>但是实际工作中，这样肯定是不行的，你会遇到各种各样的问题，如下：</p>
<ul>
<li>网络出现错误，任何错误都可能。例如机器宕了，网线断了，域名出错了，网络超时了，页面没有了，网站跳转了，服务被禁了，主机负载不够了…</li>
<li>服务器加上了限制，只让常见浏览器访问</li>
<li>需要登录才能抓取数据</li>
<li>IP被封掉或者IP访问次数受到限制</li>
<li>服务器加上了防盗链的限制</li>
<li>某些网站不管你HTTP请求里有没有Accept-Encoding头部，也不管你头部具体内容是什么，反正总给你发gzip后的内容</li>
<li>URL链接千奇百怪，带汉字的也罢了，有的甚至还有回车换行</li>
<li>某些网站HTTP头部里有一个Content-Type，网页里有好几个Content-Type，更过分的是，各个Content-Type还不一样，最过分的是，这些Content-Type可能都不是正文里使用的Content-Type，从而导致乱码</li>
<li>需要抓取的数据很多，怎么提高抓取效率</li>
<li>网站内容通过AJAX请求加载的</li>
<li>需要验证码识别<br>至于怎么解决上述问题，我们一一道来。</li>
</ul>
<p>2、网络或网站出现错误等处理</p>
<p>之前我搞过百度音乐的爬虫，因为当时百度音乐有个bug，就是可以任何人都下载百度音乐的无损、高品质音乐，我就写了个爬虫，开了几十个进程抓取下载地址（抓包解析JSON串数据）存储到数据库里，虽然百度无ip访问限制这些麻烦事，但是程序无任何错误处理的情况下，依然出现各自异常错误，所以我们只需要捕捉异常，处理异常就可以了。我们可以根据urlopen的返回值，读取它的HTTP状态码。除此之外，我们在urlopen的时候，也需要设置timeout参数，以保证处理好超时。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line">import <span class="keyword">socket</span></div><div class="line"></div><div class="line">try:</div><div class="line"> f = urllib2.urlopen(<span class="string">'http://armsword'</span>, timeout = <span class="number">10</span>)</div><div class="line"> code = f.getcode()</div><div class="line"> <span class="keyword">if</span> code &<span class="keyword">lt</span>; <span class="number">200</span> <span class="keyword">or</span> code &<span class="keyword">gt</span>;= <span class="number">300</span>:</div><div class="line"> <span class="comment">#你自己的HTTP错误处理</span></div><div class="line">except Exception, e:</div><div class="line"> <span class="keyword">if</span> isinstance(e, urllib2.HTTPError):</div><div class="line"> <span class="keyword">print</span> <span class="string">'http error: {0}'</span>.<span class="keyword">format</span>(e.code)</div><div class="line"> elif isinstance(e, urllib2.URLError) <span class="keyword">and</span> isinstance(e.reason, <span class="keyword">socket</span>.timeout):</div><div class="line"> <span class="keyword">print</span> <span class="string">'url error: socket timeout {0}'</span>.<span class="keyword">format</span>(e.__str_<span class="number">_</span>())</div><div class="line"> <span class="keyword">else</span>:</div><div class="line"> <span class="keyword">print</span> <span class="string">'error: '</span> + e.__str_<span class="number">_</span>()</div></pre></td></tr></table></figure>

<p>3、服务器做了限制</p>
<p>3.1、防盗连限制</p>
<p>某些站点有所谓的反盗链设置，其实说穿了很简单，就是检查你发送请求的header里面，referer站点是不是他自己，我们只要把headers的referer改成该网站即可，以V2EX的领取每日奖励地址为例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="variable">header =</span> {'Referer':'http://www.v2ex.com/signin'}</div><div class="line"><span class="variable">req =</span> urllib2.Request('http://www.v2ex.com',<span class="variable">headers =</span> header)</div><div class="line"><span class="variable">response =</span> urllib2.urlopen(<span class="variable">url =</span> req,<span class="variable">timeout =</span> <span class="number">10</span>).read</div><div class="line"><span class="comment">#response = urllib2.urlopen(req)).read()</span></div></pre></td></tr></table></figure>

<p>3.2、限制浏览器登录，需伪装成浏览器</p>
<p>某些网站做了限制，进制爬虫的访问，此时我们可以更改HTTP的header，与3.1相似：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="variable">header =</span> {&quot;User-Agent&quot;:&quot;Mozilla/<span class="number">5.0</span> (X11; Linux i686) AppleWebKit/<span class="number">537.36</span></div><div class="line">(KHTML, like Gecko) Ubuntu Chromium/<span class="number">31.0</span>.<span class="number">1650.63</span> Chrome/<span class="number">31.0</span>.<span class="number">1650.63</span> Safari/<span class="number">537.36</span>&quot;}</div><div class="line"><span class="variable">req =</span> urllib2.Request(url,<span class="variable">headers=</span>header)</div></pre></td></tr></table></figure>

<p>3.3、IP访问次数受到限制</p>
<p>如果IP被封掉、或者访问次数受到限制，使用代理服务器比较有用，代理IP可以自己抓取一些代理网站的数据：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">proxy_ip = urllib2.ProxyHandler({<span class="symbol">'htt</span>p':<span class="symbol">'http</span>:<span class="comment">//XX.XX.XX.XX:XXXX'}) #XX为代理IP和端口</span></div><div class="line">opener = urllib2.build_opener(proxy_ip, urllib2.HTTPHandler)</div><div class="line">urllib2.install_opener(opener)</div><div class="line">content = urllib2.urlopen(<span class="symbol">'http</span>:<span class="comment">//XXXX.com').read()</span></div></pre></td></tr></table></figure>

<p>4、需要登录才能抓取数据</p>
<p>4.1、表单的处理</p>
<p>抓包，Chrome或者Firefox+Httpfox浏览器插件就能办到，查看POST数据，模拟表单就可以了，这里不再细说，这里主要注意，在GET/POST一些数据的时候，需要对GET/POST的数据做些编码，即使用urlencode()，我们以北邮校园网关登录为例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">postdata = {</div><div class="line"> <span class="string">'DDDDD'</span>: uname, <span class="comment">#用户名</span></div><div class="line"> <span class="string">'upass'</span>: u_pass, <span class="comment">#密码</span></div><div class="line"> <span class="string">'R1'</span>: <span class="number">0</span>, <span class="comment">#其他参数</span></div><div class="line"> <span class="string">'R2'</span>: <span class="number">1</span>,</div><div class="line"> <span class="string">'para'</span>: <span class="number">00</span>,</div><div class="line"> <span class="string">'0MKKey'</span>: <span class="number">123456</span></div><div class="line"> }</div><div class="line"></div><div class="line">en_url = urllib.urlencode(postdata)</div></pre></td></tr></table></figure>

<p>4.2、Cookie处理</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">#获取一个cookie对象</span></div><div class="line">cookie = cookielib.CookieJar()</div><div class="line"><span class="preprocessor">#构建cookie处理器</span></div><div class="line">cookie_p = urllib2.HTTPCookieProcessor(cookie)</div><div class="line"><span class="preprocessor">#装载cookie</span></div><div class="line">opener = urllib2.build_opener(cookie_p)</div><div class="line"><span class="preprocessor">#注意此处后面的install_opener(opener),安装不同的opener对象作为urlopen() 使用的全局URL opener</span></div><div class="line">urllib2.install_opener(opener)</div><div class="line">content = urllib2.urlopen(<span class="string">'http://armsword.com'</span>).read()</div></pre></td></tr></table></figure>

<p>如果同时需要代理和Cookie，这样就可以：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">opener </span>=<span class="string"> urllib2.build_opener(proxy_ip, cookie_p, urllib2.HTTPHandler)</span></div></pre></td></tr></table></figure>

<p>如果上面未使用install_opener(opener)，则使用：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response = opener.<span class="keyword">open</span>(url).<span class="keyword">read</span>()</div></pre></td></tr></table></figure>

<p>以V2EX每日签到为例，把上面的流程走一遍：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#全局变量装载Cookie省略</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></div><div class="line"> <span class="string">'''</span></div><div class="line"> once值每次登录都不一样，在页面上可以看到</div><div class="line"> 也必需有http header</div><div class="line"> '''</div><div class="line"> req = urllib2.Request(url_login)</div><div class="line"> once = get_info(req,<span class="string">'name'</span>,<span class="string">'once'</span>)[<span class="string">'value'</span>] <span class="comment">#省略</span></div><div class="line"> postdata = {</div><div class="line"> <span class="string">'u'</span>:username,</div><div class="line"> <span class="string">'p'</span>:password,</div><div class="line"> <span class="string">'once'</span>:once,</div><div class="line"> <span class="comment">#'next':&quot;/&quot;</span></div><div class="line"> }</div><div class="line"></div><div class="line"> header = {<span class="string">'Host'</span>:<span class="string">'www.v2ex.com'</span>,<span class="string">'Origin'</span>:<span class="string">'http://www.v2ex.com'</span>,</div><div class="line"><span class="string">'Referer'</span>:<span class="string">'http://www.v2ex.com/signin'</span>,</div><div class="line"><span class="string">'User-Agent'</span>:&quot;Mozilla/<span class="number">5.0</span> (X11; Linux i686) AppleWebKit/<span class="number">537.36</span></div><div class="line">(KHTML, like Gecko) Ubuntu Chromium/<span class="number">32.0</span><span class="number">.1700</span><span class="number">.107</span> Chrome/<span class="number">32.0</span><span class="number">.1700</span><span class="number">.107</span> Safari/<span class="number">537.36</span>&quot;}</div><div class="line"></div><div class="line"> data = urllib.urlencode(postdata)</div><div class="line"></div><div class="line"> req = urllib2.Request(url_login,data,header)</div><div class="line"> response = opener.open(req)</div></pre></td></tr></table></figure>

<p>5、gzip/deflate支持</p>
<p>某些网站不管你请求头带不带Accept-Encoding:gzip，它返回的都是gzip压缩的内容，urlopen不会处理gzip压缩的内容，所以得到的就是乱码，以下为解决方法：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="variable">f =</span> urllib2.urlopen(url)</div><div class="line"><span class="variable">headers =</span> f.info()</div><div class="line"><span class="variable">rawdata =</span> f.read()</div><div class="line"><span class="keyword">if</span> ('Content-Encoding' <span class="keyword">in</span> headers <span class="constant">and</span> headers['Content-Encoding']) <span class="constant">or</span> \</div><div class="line"> ('content-encoding' <span class="keyword">in</span> headers <span class="constant">and</span> headers['content-encoding']):</div><div class="line"> <span class="built_in">import</span> gzip</div><div class="line"> <span class="built_in">import</span> StringIO</div><div class="line"> <span class="variable">data =</span> StringIO.StringIO(rawdata)</div><div class="line"> <span class="variable">gz =</span> gzip.GzipFile(<span class="variable">fileobj=</span>data)</div><div class="line"> <span class="variable">rawdata =</span> gz.read()</div><div class="line"> gz.close()</div></pre></td></tr></table></figure>

<p>6、URL编码和网页编码处理<br>如果URL里含有中文，要对相应的中文进行编码，可以使用urllib.quote(‘要编码的字符串’)，如下所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">http</span>://www.google.com/<span class="comment">#newwindow=1&amp;q=你好</span></div><div class="line">query = urllib.<span class="constant">quote</span>(<span class="string">'你好'</span>)</div><div class="line">url = <span class="string">'http://www.google.com/#newwindow=1&amp;q='</span>+query</div><div class="line">response = urllib.urlopen(url).<span class="built_in">read</span>()</div></pre></td></tr></table></figure>

<p>至于处理网页编码，按照一般浏览器的处理流程，判断网页的编码首先是根据HTTP服务器发过来的HTTP响应头部中Content-Type字段，例如text/html; charset=utf-8就表示这是一个HTML网页，使用的是utf8编码。如果HTTP头部中没有，或者网页内容的head区中有charset属性或者其http-equiv属性为Content-Type的meta元素，如果有charset属性，则直接读取这个属性，如果是后者，则读取这个元素的content属性。但是实际情况是许多网页不按照规矩出牌，首先，HTTP响应里不一定有Content-Type头部，其次某些网页里可能没有Content-Type或者有多个Content-Type(例如百度搜索的缓存页面)。所以，我的经验基本是自己多猜测几次吧。其实GBK、UTF-8占了绝大部分，使用xx.decode(‘gbk’,’ignore’).encode(‘utf-8’) （或相反）试试，其中ignore是指忽略非法字符。</p>
<p>7、AJAX</p>
<p>AJAX 是一种用于创建快速动态网页的技术。<br>通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。<br>传统的网页（不使用 AJAX）如果需要更新内容，必需重载整个网页面。</p>
<p>AJAX依然是HTTP请求，只是来自页面发起的另一个HTTP请求。我之前做百度音乐的时候，在chrome下的network选项分析每一个选项，找到对应的ajax数据，分析json，在正则、查找之类的。</p>
<p>当然，也可以通过selenium、phantomjs、mechanize等模拟浏览器引擎构建一个真实的浏览器执行JS、渲染页面（在写这篇文章时，我还没用到过这些），这应该算是终极大招了吧。</p>
<p>8、效率</p>
<p>目前使用过多线程、多进程，多进程注意下僵尸进程。</p>
<p>异步：用twisted进行异步I/O抓取，tornado（如果拿到实习offer后，我可能会抽出几天时间学习这个，我感觉我应该学习下web编程，纠结了下flask还是tornado，其实无论哪个一周完全可以入门了，python你懂的。）</p>
<p>9、验证码</p>
<p>图像识别方面的知识，好吧，等我学会后再写吧。。。</p>
<p>根据我的经验，网络爬虫里网络带宽是主要瓶颈，上文只说了抓取，其实网页抽取依然有很多知识要做的，我现在会的都是些皮毛，要学的还很多，等拿到dream 公司的实习offer后，有空继续折腾下，现在主要精力是弥补自己的一些知识缺点和面试长考的问题。</p>
<p>参考链接：</p>
<p><a href="http://blog.binux.me/2013/09/howto-crawl-web/" target="_blank" rel="external">http://blog.binux.me/2013/09/howto-crawl-web/</a></p>
<p><a href="http://blog.raphaelzhang.com/2012/03/issues-in-python-crawler/" target="_blank" rel="external">http://blog.raphaelzhang.com/2012/03/issues-in-python-crawler/</a></p>
<p><a href="http://yxmhero1989.blog.163.com/blog/static/112157956201311821444664/" target="_blank" rel="external">http://yxmhero1989.blog.163.com/blog/static/112157956201311821444664/</a></p>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/ML-NLP/">ML/NLP</a>
  </div>

        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>

<!-- Duoshuo Comment BEGIN -->
<div class="ds-thread" data-title="使用python编写简单网络爬虫技巧总结" data-url="http://armsword.com/2014/03/31/python-in-crawler/"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"armsword"};
(function() {
        var ds = document.createElement('script');
            ds.type = 'text/javascript';ds.async = true;
                ds.src = 'http://static.duoshuo.com/embed.js';
                    ds.charset = 'UTF-8';
                        (document.getElementsByTagName('head')[0] 
                            || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script>
<!-- Duoshuo Comment END -->


</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:armsword.com">
  </form>
</div>


  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/C-C/">C/C++</a><small>22</small></li>
  
    <li><a href="/categories/Database/">Database</a><small>2</small></li>
  
    <li><a href="/categories/Linux/">Linux</a><small>13</small></li>
  
    <li><a href="/categories/ML-NLP/">ML/NLP</a><small>3</small></li>
  
    <li><a href="/categories/Python/">Python</a><small>2</small></li>
  
    <li><a href="/categories/SDK-MFC/">SDK/MFC</a><small>3</small></li>
  
    <li><a href="/categories/我的生活/">我的生活</a><small>4</small></li>
  
    <li><a href="/categories/算法-数据结构/">算法/数据结构</a><small>4</small></li>
  
    <li><a href="/categories/计算机网络/">计算机网络</a><small>9</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">最新文章</h3>
  <ul class="entry">
    
      <li>
        <a href="/2014/12/01/learn-emacs-note/">Emacs学习笔记</a>
      </li>
    
      <li>
        <a href="/2014/11/22/move-from-wordpress-to-hexo/">将WordPress迁移到hexo并同时托管到Github和Gitcafe上</a>
      </li>
    
      <li>
        <a href="/2014/11/07/finding-job-result/">一蓑烟雨任平生</a>
      </li>
    
      <li>
        <a href="/2014/10/29/tinyhttpd-code-analyse/">tinyhttpd源码剖析</a>
      </li>
    
      <li>
        <a href="/2014/10/26/webbench-source-analyse/">Webbench源码剖析</a>
      </li>
    
      <li>
        <a href="/2014/09/04/redis-ae-analyse/">Redis AE事件库源码剖析</a>
      </li>
    
      <li>
        <a href="/2014/08/30/find-the-invaded-reason/">记一次查找实验室服务器被诡异的入侵原因</a>
      </li>
    
  </ul>
</div>



  

  <div class="widget tag">
<h3 class="title">友情链接</h3>
<ul class="entry">
<li><a href="http://linuxer.me" target="_blank" title="armsword的涅槃之地">armsword</a></li>
<li><a href="http://yanyiwu.com" target="_blank" title="YanYi">YanYi</a></li>
<li><a href="http://blog.purplecow.me" target="_blank" title="PurpleCow">PurpleCow</a></li>
<li><a href="http://cstdlib.com" target="_blank" title="徐小夫">徐小夫</a></li>
</ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft"> 
  
  &copy; 2014 armsword
  
  
<script type="text/javascript" src="http://tajs.qq.com/stats?sId=39010465" charset="UTF-8"></script>


</div>

<div class="clearfix"></div>
</footer>
  <script src="/js/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:30px;right:30px;cursor: pointer;">
<a title="返回顶部"><img src="/imgs/scrollup.png"/></a>
</div>

<script src="/js/totop.js"></script>


</body>
</html>